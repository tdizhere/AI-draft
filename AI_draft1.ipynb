{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1676841,
          "sourceType": "datasetVersion",
          "datasetId": 993371
        }
      ],
      "dockerImageVersionId": 30804,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "AI-draft1",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tdizhere/AI-draft/blob/main/AI_draft1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "projjal1_human_conversation_training_data_path = kagglehub.dataset_download('projjal1/human-conversation-training-data')\n",
        "\n",
        "print('Data source import complete.',projjal1_human_conversation_training_data_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osbdbz5om7oI",
        "outputId": "cad8d545-81e3-4ff3-d16c-71904122be08"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/projjal1/human-conversation-training-data?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41.6k/41.6k [00:00<00:00, 18.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Data source import complete. /root/.cache/kagglehub/datasets/projjal1/human-conversation-training-data/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"grafstor/simple-dialogs-for-chatbot\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Yp_szh05VMm",
        "outputId": "6ce9f744-eb2e-41ac-bc47-eac7173e25be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/grafstor/simple-dialogs-for-chatbot/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
        "import gc  # Garbage collection\n",
        "\n",
        "# Set up paths\n",
        "data_path = \"/root/.cache/kagglehub/datasets/grafstor/simple-dialogs-for-chatbot/versions/2/dialogs.txt\"\n",
        "\n",
        "# Check for GPU availability and memory\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Memory monitoring function\n",
        "def print_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "        print(f\"Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "\n",
        "# Load tokenizer first with limited vocabulary\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",\n",
        "    add_prefix_space=True,  # More efficient tokenization\n",
        "    max_model_input_sizes={'gpt2': 1024}  # Limit input size\n",
        ")\n",
        "\n",
        "# Ensure padding token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"Padding token set to: {tokenizer.pad_token}\")\n",
        "\n",
        "# Load dataset with memory-efficient approach\n",
        "print(\"Loading dataset...\")\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length=64, max_samples=5000):  # Reduced defaults\n",
        "        self.examples = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                chat_lines = f.readlines()\n",
        "\n",
        "            # Limit number of samples to prevent memory overload\n",
        "            chat_lines = chat_lines[:max_samples*2]\n",
        "\n",
        "            # Pair every two consecutive lines as input and target\n",
        "            for i in range(0, len(chat_lines) - 1, 2):\n",
        "                input_text = chat_lines[i].strip()\n",
        "                target_text = chat_lines[i + 1].strip()\n",
        "\n",
        "                # Encode inputs and targets as a single sequence\n",
        "                encoded = tokenizer(\n",
        "                    f\"{input_text} {tokenizer.eos_token} {target_text}\",\n",
        "                    max_length=max_length,\n",
        "                    truncation=True,\n",
        "                    padding=\"max_length\",\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "                self.examples.append({\n",
        "                    \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
        "                    \"attention_mask\": encoded[\"attention_mask\"].squeeze(),\n",
        "                })\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File not found at {file_path}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset: {e}\")\n",
        "            raise\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "# Initialize dataset with smaller batch size and fewer samples\n",
        "dataset = ChatDataset(data_path, tokenizer, max_length=64, max_samples=5000)\n",
        "\n",
        "# DataLoader with reduced batch size and workers\n",
        "batch_size = 4  # Reduced batch size\n",
        "data_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,  # Use multiple workers if possible\n",
        "    pin_memory=True  # Faster data transfer to GPU\n",
        ")\n",
        "\n",
        "# Clear GPU memory before model loading\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print_gpu_memory()\n",
        "\n",
        "# Load model with lower precision to save memory\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", torch_dtype=torch.float16)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.to(device)\n",
        "\n",
        "print_gpu_memory()\n",
        "\n",
        "# Optimizer with lower learning rate\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
        "\n",
        "# Training loop with safety checks\n",
        "epochs = 3\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Clear memory before each epoch\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        try:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=input_ids,\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print progress and clear memory periodically\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"CUDA out of memory error: {e}\")\n",
        "            # Clear memory and try to continue\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            continue\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {epoch_loss / len(data_loader)}\")\n",
        "\n",
        "# Save model\n",
        "model_save_path = \"/conversational_model\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"Model saved at {model_save_path}\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-09T13:07:06.960002Z",
          "iopub.execute_input": "2024-12-09T13:07:06.960457Z",
          "iopub.status.idle": "2024-12-09T13:07:39.114397Z",
          "shell.execute_reply.started": "2024-12-09T13:07:06.960416Z",
          "shell.execute_reply": "2024-12-09T13:07:39.112831Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gyCalODm7oI",
        "outputId": "87a8e736-aec3-4594-8c3f-4e8dc9428892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading tokenizer...\n",
            "Padding token set to: <|endoftext|>\n",
            "Loading dataset...\n",
            "Allocated: 1.08 GB\n",
            "Cached: 1.18 GB\n",
            "Allocated: 1.35 GB\n",
            "Cached: 1.38 GB\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 0, Loss: 6.44921875\n",
            "Epoch 1, Batch 10, Loss: 2.265625\n",
            "Epoch 1, Batch 20, Loss: 2.236328125\n",
            "Epoch 1, Batch 30, Loss: 1.4580078125\n",
            "Epoch 1, Batch 40, Loss: 1.8828125\n",
            "Epoch 1, Batch 50, Loss: 1.4794921875\n",
            "Epoch 1, Batch 60, Loss: 1.48046875\n",
            "Epoch 1, Batch 70, Loss: 1.7333984375\n",
            "Epoch 1, Batch 80, Loss: 1.44921875\n",
            "Epoch 1, Batch 90, Loss: 1.38671875\n",
            "Epoch 1, Batch 100, Loss: 1.2646484375\n",
            "Epoch 1, Batch 110, Loss: 1.9501953125\n",
            "Epoch 1, Batch 120, Loss: 1.2958984375\n",
            "Epoch 1, Batch 130, Loss: 1.443359375\n",
            "Epoch 1, Batch 140, Loss: 1.509765625\n",
            "Epoch 1, Batch 150, Loss: 1.8916015625\n",
            "Epoch 1, Batch 160, Loss: 1.576171875\n",
            "Epoch 1, Batch 170, Loss: 1.2646484375\n",
            "Epoch 1, Batch 180, Loss: 1.244140625\n",
            "Epoch 1, Batch 190, Loss: 1.921875\n",
            "Epoch 1, Batch 200, Loss: 1.4267578125\n",
            "Epoch 1, Batch 210, Loss: 1.55078125\n",
            "Epoch 1, Batch 220, Loss: 1.5556640625\n",
            "Epoch 1, Batch 230, Loss: 1.0390625\n",
            "Epoch 1, Batch 240, Loss: 1.9970703125\n",
            "Epoch 1, Batch 250, Loss: 1.7119140625\n",
            "Epoch 1, Batch 260, Loss: 1.494140625\n",
            "Epoch 1, Batch 270, Loss: 2.08984375\n",
            "Epoch 1, Batch 280, Loss: 1.6640625\n",
            "Epoch 1, Batch 290, Loss: 1.7099609375\n",
            "Epoch 1, Batch 300, Loss: 1.89453125\n",
            "Epoch 1, Batch 310, Loss: 1.640625\n",
            "Epoch 1, Batch 320, Loss: 1.40234375\n",
            "Epoch 1, Batch 330, Loss: 1.6669921875\n",
            "Epoch 1, Batch 340, Loss: 1.1005859375\n",
            "Epoch 1, Batch 350, Loss: 1.888671875\n",
            "Epoch 1, Batch 360, Loss: 1.046875\n",
            "Epoch 1, Batch 370, Loss: 1.67578125\n",
            "Epoch 1, Batch 380, Loss: 1.5888671875\n",
            "Epoch 1, Batch 390, Loss: 1.4990234375\n",
            "Epoch 1, Batch 400, Loss: 1.5234375\n",
            "Epoch 1, Batch 410, Loss: 1.4619140625\n",
            "Epoch 1, Batch 420, Loss: 1.9130859375\n",
            "Epoch 1, Batch 430, Loss: 1.255859375\n",
            "Epoch 1, Batch 440, Loss: 1.6328125\n",
            "Epoch 1, Batch 450, Loss: 1.55078125\n",
            "Epoch 1, Batch 460, Loss: 1.453125\n",
            "Epoch 1/3, Average Loss: 1.6632294879962446\n",
            "Epoch 2, Batch 0, Loss: 1.0537109375\n",
            "Epoch 2, Batch 10, Loss: 0.95556640625\n",
            "Epoch 2, Batch 20, Loss: 1.447265625\n",
            "Epoch 2, Batch 30, Loss: 1.6220703125\n",
            "Epoch 2, Batch 40, Loss: 0.88232421875\n",
            "Epoch 2, Batch 50, Loss: 1.0185546875\n",
            "Epoch 2, Batch 60, Loss: 1.3349609375\n",
            "Epoch 2, Batch 70, Loss: 1.33203125\n",
            "Epoch 2, Batch 80, Loss: 1.177734375\n",
            "Epoch 2, Batch 90, Loss: 1.3720703125\n",
            "Epoch 2, Batch 100, Loss: 1.126953125\n",
            "Epoch 2, Batch 110, Loss: 1.16796875\n",
            "Epoch 2, Batch 120, Loss: 1.4453125\n",
            "Epoch 2, Batch 130, Loss: 1.486328125\n",
            "Epoch 2, Batch 140, Loss: 1.0693359375\n",
            "Epoch 2, Batch 150, Loss: 1.12890625\n",
            "Epoch 2, Batch 160, Loss: 1.125\n",
            "Epoch 2, Batch 170, Loss: 1.7802734375\n",
            "Epoch 2, Batch 180, Loss: 1.193359375\n",
            "Epoch 2, Batch 190, Loss: 1.310546875\n",
            "Epoch 2, Batch 200, Loss: 0.88232421875\n",
            "Epoch 2, Batch 210, Loss: 1.1689453125\n",
            "Epoch 2, Batch 220, Loss: 1.3837890625\n",
            "Epoch 2, Batch 230, Loss: 0.94482421875\n",
            "Epoch 2, Batch 240, Loss: 0.892578125\n",
            "Epoch 2, Batch 250, Loss: 1.427734375\n",
            "Epoch 2, Batch 260, Loss: 1.3330078125\n",
            "Epoch 2, Batch 270, Loss: 1.048828125\n",
            "Epoch 2, Batch 280, Loss: 1.1865234375\n",
            "Epoch 2, Batch 290, Loss: 1.205078125\n",
            "Epoch 2, Batch 300, Loss: 1.1455078125\n",
            "Epoch 2, Batch 310, Loss: 1.1328125\n",
            "Epoch 2, Batch 320, Loss: 1.3076171875\n",
            "Epoch 2, Batch 330, Loss: 1.3564453125\n",
            "Epoch 2, Batch 340, Loss: 1.4697265625\n",
            "Epoch 2, Batch 350, Loss: 1.06640625\n",
            "Epoch 2, Batch 360, Loss: 1.8720703125\n",
            "Epoch 2, Batch 370, Loss: 1.2138671875\n",
            "Epoch 2, Batch 380, Loss: 1.09375\n",
            "Epoch 2, Batch 390, Loss: 1.419921875\n",
            "Epoch 2, Batch 400, Loss: 1.23828125\n",
            "Epoch 2, Batch 410, Loss: 1.240234375\n",
            "Epoch 2, Batch 420, Loss: 1.3388671875\n",
            "Epoch 2, Batch 430, Loss: 1.0146484375\n",
            "Epoch 2, Batch 440, Loss: 1.4521484375\n",
            "Epoch 2, Batch 450, Loss: 0.94775390625\n",
            "Epoch 2, Batch 460, Loss: 1.5302734375\n",
            "Epoch 2/3, Average Loss: 1.224085468079399\n",
            "Epoch 3, Batch 0, Loss: 0.91162109375\n",
            "Epoch 3, Batch 10, Loss: 0.896484375\n",
            "Epoch 3, Batch 20, Loss: 1.125\n",
            "Epoch 3, Batch 30, Loss: 0.83837890625\n",
            "Epoch 3, Batch 40, Loss: 0.96923828125\n",
            "Epoch 3, Batch 50, Loss: 1.244140625\n",
            "Epoch 3, Batch 60, Loss: 0.8623046875\n",
            "Epoch 3, Batch 70, Loss: 0.5888671875\n",
            "Epoch 3, Batch 80, Loss: 0.86962890625\n",
            "Epoch 3, Batch 90, Loss: 0.89501953125\n",
            "Epoch 3, Batch 100, Loss: 0.90380859375\n",
            "Epoch 3, Batch 110, Loss: 1.00390625\n",
            "Epoch 3, Batch 120, Loss: 1.189453125\n",
            "Epoch 3, Batch 130, Loss: 0.7802734375\n",
            "Epoch 3, Batch 140, Loss: 1.0576171875\n",
            "Epoch 3, Batch 150, Loss: 1.0830078125\n",
            "Epoch 3, Batch 160, Loss: 1.298828125\n",
            "Epoch 3, Batch 170, Loss: 0.8935546875\n",
            "Epoch 3, Batch 180, Loss: 0.82470703125\n",
            "Epoch 3, Batch 190, Loss: 0.84326171875\n",
            "Epoch 3, Batch 200, Loss: 1.0458984375\n",
            "Epoch 3, Batch 210, Loss: 1.00390625\n",
            "Epoch 3, Batch 220, Loss: 1.0185546875\n",
            "Epoch 3, Batch 230, Loss: 0.79150390625\n",
            "Epoch 3, Batch 240, Loss: 0.78564453125\n",
            "Epoch 3, Batch 250, Loss: 1.1396484375\n",
            "Epoch 3, Batch 260, Loss: 0.875\n",
            "Epoch 3, Batch 270, Loss: 1.017578125\n",
            "Epoch 3, Batch 280, Loss: 1.0927734375\n",
            "Epoch 3, Batch 290, Loss: 1.115234375\n",
            "Epoch 3, Batch 300, Loss: 1.2021484375\n",
            "Epoch 3, Batch 310, Loss: 0.955078125\n",
            "Epoch 3, Batch 320, Loss: 1.0849609375\n",
            "Epoch 3, Batch 330, Loss: 1.203125\n",
            "Epoch 3, Batch 340, Loss: 1.2265625\n",
            "Epoch 3, Batch 350, Loss: 1.0263671875\n",
            "Epoch 3, Batch 360, Loss: 0.9111328125\n",
            "Epoch 3, Batch 370, Loss: 0.91650390625\n",
            "Epoch 3, Batch 380, Loss: 1.0830078125\n",
            "Epoch 3, Batch 390, Loss: 0.8720703125\n",
            "Epoch 3, Batch 400, Loss: 1.0302734375\n",
            "Epoch 3, Batch 410, Loss: 1.0087890625\n",
            "Epoch 3, Batch 420, Loss: 0.85009765625\n",
            "Epoch 3, Batch 430, Loss: 0.87939453125\n",
            "Epoch 3, Batch 440, Loss: 1.0234375\n",
            "Epoch 3, Batch 450, Loss: 1.314453125\n",
            "Epoch 3, Batch 460, Loss: 1.0380859375\n",
            "Epoch 3/3, Average Loss: 0.9781352685756438\n",
            "Model saved at /conversational_model\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "import gc\n",
        "import json\n",
        "\n",
        "# Configuration\n",
        "CONFIG_PATH = \"/conversational_model/training_config.json\"\n",
        "\n",
        "# Utility function to save training state\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, loss, checkpoint_path):\n",
        "    \"\"\"Save model checkpoint with training state.\"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "# Utility function to load checkpoint\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, scheduler, device):\n",
        "    \"\"\"Load model checkpoint and training state.\"\"\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    return checkpoint['epoch'], checkpoint['loss']\n",
        "\n",
        "# Enhanced Dataset with more robust loading\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length=128, max_samples=None):\n",
        "        self.examples = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                chat_lines = f.readlines()\n",
        "\n",
        "            # Optional sample limit\n",
        "            if max_samples:\n",
        "                chat_lines = chat_lines[:max_samples*2]\n",
        "\n",
        "            # Process lines into training examples\n",
        "            for i in range(0, len(chat_lines) - 1, 2):\n",
        "                input_text = chat_lines[i].strip()\n",
        "                target_text = chat_lines[i + 1].strip()\n",
        "\n",
        "                # Combine input and target with special token\n",
        "                combined_text = f\"{input_text} {tokenizer.eos_token} {target_text}\"\n",
        "\n",
        "                # Tokenize\n",
        "                encoded = tokenizer(\n",
        "                    combined_text,\n",
        "                    max_length=max_length,\n",
        "                    truncation=True,\n",
        "                    padding=\"max_length\",\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "                self.examples.append({\n",
        "                    \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
        "                    \"attention_mask\": encoded[\"attention_mask\"].squeeze(),\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset: {e}\")\n",
        "            raise\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "# Model Training Function\n",
        "def train_model(\n",
        "    data_path,\n",
        "    model_save_dir='/conversational_model',\n",
        "    resume_training=False,\n",
        "    max_epochs=5,\n",
        "    batch_size=4\n",
        "):\n",
        "    # Ensure save directories exist\n",
        "    os.makedirs(model_save_dir, exist_ok=True)\n",
        "    checkpoint_dir = os.path.join(model_save_dir, 'checkpoints')\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Device setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Tokenizer setup\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Dataset preparation\n",
        "    full_dataset = ChatDataset(data_path, tokenizer, max_length=128, max_samples=10000)\n",
        "\n",
        "    # Split dataset\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    val_size = len(full_dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Model setup\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "    total_steps = len(train_loader) * max_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1*total_steps),\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Check for existing checkpoint\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    if resume_training:\n",
        "        checkpoint_files = sorted(\n",
        "            [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_')],\n",
        "            key=lambda x: int(x.split('_')[1].split('.')[0])\n",
        "        )\n",
        "\n",
        "        if checkpoint_files:\n",
        "            latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[-1])\n",
        "            start_epoch, best_val_loss = load_checkpoint(\n",
        "                latest_checkpoint, model, optimizer, scheduler, device\n",
        "            )\n",
        "            print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, max_epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=input_ids\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Periodic checkpoint\n",
        "            if batch_idx % 100 == 0:\n",
        "                avg_train_loss = np.mean(train_losses)\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Train Loss: {avg_train_loss}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=input_ids\n",
        "                )\n",
        "                val_losses.append(outputs.loss.item())\n",
        "\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss}, Validation Loss = {avg_val_loss}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_{epoch+1}.pt')\n",
        "        save_checkpoint(model, optimizer, scheduler, epoch+1, avg_val_loss, checkpoint_path)\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_path = os.path.join(model_save_dir, 'best_model')\n",
        "            model.save_pretrained(best_model_path)\n",
        "            tokenizer.save_pretrained(best_model_path)\n",
        "            print(f\"Best model saved with validation loss: {best_val_loss}\")\n",
        "\n",
        "    # Final model save\n",
        "    model.save_pretrained(model_save_dir)\n",
        "    tokenizer.save_pretrained(model_save_dir)\n",
        "    print(\"Training completed and model saved.\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Inference Function\n",
        "def chat_with_model(model, tokenizer, prompt, max_length=100, num_return_sequences=1, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize input\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "            no_repeat_ngram_size=2,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "    # Decode responses\n",
        "    responses = [tokenizer.decode(seq, skip_special_tokens=True) for seq in output]\n",
        "    return responses\n",
        "\n",
        "# Testing Function\n",
        "def test_model(model, tokenizer, test_prompts):\n",
        "    print(\"\\n--- Model Testing ---\")\n",
        "    for prompt in test_prompts:\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        responses = chat_with_model(model, tokenizer, prompt)\n",
        "        for i, response in enumerate(responses, 1):\n",
        "            print(f\"Response {i}: {response}\")\n",
        "\n",
        "# Main Execution\n",
        "def main():\n",
        "    # Set paths\n",
        "    data_path = \"/root/.cache/kagglehub/datasets/projjal1/human-conversation-training-data/versions/1/human_chat.txt\"\n",
        "    model_save_dir = \"/conversational_model\"\n",
        "\n",
        "    # Train model\n",
        "    trained_model, trained_tokenizer = train_model(\n",
        "        data_path,\n",
        "        model_save_dir,\n",
        "        resume_training=True,  # Will resume from last checkpoint if exists\n",
        "        max_epochs=5,\n",
        "        batch_size=4\n",
        "    )\n",
        "\n",
        "    # Test prompts\n",
        "    test_prompts = [\n",
        "        \"Hello, how are you?\",\n",
        "        \"What is the meaning of life?\",\n",
        "        \"Tell me a short story.\",\n",
        "        \"Explain quantum computing in simple terms.\"\n",
        "    ]\n",
        "\n",
        "    # Test the model\n",
        "    test_model(trained_model, trained_tokenizer, test_prompts)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-09T13:09:33.068611Z",
          "iopub.execute_input": "2024-12-09T13:09:33.069034Z",
          "iopub.status.idle": "2024-12-09T13:10:48.526389Z",
          "shell.execute_reply.started": "2024-12-09T13:09:33.069Z",
          "shell.execute_reply": "2024-12-09T13:10:48.52496Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kODyTSEm7oJ",
        "outputId": "c539968e-8824-49c1-c77e-c3c503fc3976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 0, Train Loss: 7.369625568389893\n",
            "Epoch 1, Batch 100, Train Loss: 2.2544064152948926\n",
            "Epoch 1: Train Loss = 1.825354473789533, Validation Loss = 0.9632481885583777\n",
            "Checkpoint saved at /conversational_model/checkpoints/checkpoint_1.pt\n",
            "Best model saved with validation loss: 0.9632481885583777\n",
            "Epoch 2, Batch 0, Train Loss: 0.6830458045005798\n",
            "Epoch 2, Batch 100, Train Loss: 0.8678857468732513\n",
            "Epoch 2: Train Loss = 0.8804653787612915, Validation Loss = 0.9628808631708747\n",
            "Checkpoint saved at /conversational_model/checkpoints/checkpoint_2.pt\n",
            "Best model saved with validation loss: 0.9628808631708747\n",
            "Epoch 3, Batch 0, Train Loss: 0.9417645335197449\n",
            "Epoch 3, Batch 100, Train Loss: 0.8259283732069601\n",
            "Epoch 3: Train Loss = 0.7895452650388082, Validation Loss = 0.9824455090259251\n",
            "Checkpoint saved at /conversational_model/checkpoints/checkpoint_3.pt\n",
            "Epoch 4, Batch 0, Train Loss: 0.4639434814453125\n",
            "Epoch 4, Batch 100, Train Loss: 0.7235138889881644\n",
            "Epoch 4: Train Loss = 0.7355995922287305, Validation Loss = 0.9965160112631949\n",
            "Checkpoint saved at /conversational_model/checkpoints/checkpoint_4.pt\n",
            "Epoch 5, Batch 0, Train Loss: 0.6514022350311279\n",
            "Epoch 5, Batch 100, Train Loss: 0.6934931372651959\n",
            "Epoch 5: Train Loss = 0.6994798451662063, Validation Loss = 1.0065775505806271\n",
            "Checkpoint saved at /conversational_model/checkpoints/checkpoint_5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed and model saved.\n",
            "\n",
            "--- Model Testing ---\n",
            "\n",
            "Prompt: Hello, how are you?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 1: Hello, how are you? \n",
            "\n",
            "Prompt: What is the meaning of life?\n",
            "Response 1: What is the meaning of life? \n",
            "\n",
            "Prompt: Tell me a short story.\n",
            "Response 1: Tell me a short story. I'm a comic book artist. \n",
            "\n",
            "Prompt: Explain quantum computing in simple terms.\n",
            "Response 1: Explain quantum computing in simple terms. It's a very exciting idea, and I'm excited to try it out. \n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "git clone https://huggingface.co/datasets/flammenai/casual-conversation-DPO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "oO7CSLYU4Csx",
        "outputId": "abd4e50a-e8cf-4622-d7ee-2d2505b21b6f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-14-d687f01b680a>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-d687f01b680a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git clone https://huggingface.co/datasets/flammenai/casual-conversation-DPO\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***`Training`***"
      ],
      "metadata": {
        "id": "KwzyHQoHxSn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
        "import gc\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Set up paths\n",
        "data_path = \"/root/.cache/kagglehub/datasets/grafstor/simple-dialogs-for-chatbot/versions/2/dialog.txt\"\n",
        "\n",
        "# Check for GPU availability and memory\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Memory monitoring function\n",
        "def print_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "        print(f\"Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "\n",
        "# Load tokenizer first with limited vocabulary\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\",\n",
        "    add_prefix_space=True,\n",
        "    max_model_input_sizes={'gpt2': 1024}\n",
        ")\n",
        "\n",
        "# Ensure padding token is set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"Padding token set to: {tokenizer.pad_token}\")\n",
        "\n",
        "# Load dataset with memory-efficient approach\n",
        "print(\"Loading dataset...\")\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, max_length=64, max_samples=5000):\n",
        "        self.examples = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                chat_lines = f.readlines()\n",
        "\n",
        "            # Limit number of samples to prevent memory overload\n",
        "            chat_lines = chat_lines[:max_samples*2]\n",
        "\n",
        "            # Pair every two consecutive lines as input and target\n",
        "            for i in range(0, len(chat_lines) - 1, 2):\n",
        "                input_text = chat_lines[i].strip()\n",
        "                target_text = chat_lines[i + 1].strip()\n",
        "\n",
        "                # Encode inputs and targets as a single sequence\n",
        "                encoded = tokenizer(\n",
        "                    f\"{input_text} {tokenizer.eos_token} {target_text}\",\n",
        "                    max_length=max_length,\n",
        "                    truncation=True,\n",
        "                    padding=\"max_length\",\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "                self.examples.append({\n",
        "                    \"input_ids\": encoded[\"input_ids\"].squeeze(),\n",
        "                    \"attention_mask\": encoded[\"attention_mask\"].squeeze(),\n",
        "                })\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: File not found at {file_path}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset: {e}\")\n",
        "            raise\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]\n",
        "\n",
        "# Initialize dataset with smaller batch size and fewer samples\n",
        "dataset = ChatDataset(data_path, tokenizer, max_length=64, max_samples=5000)\n",
        "\n",
        "# DataLoader with optimized batch size and workers\n",
        "batch_size = 8  # Increase batch size if GPU allows\n",
        "data_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,  # More workers for faster data loading\n",
        "    pin_memory=True  # Faster data transfer to GPU\n",
        ")\n",
        "\n",
        "# Clear GPU memory before model loading\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print_gpu_memory()\n",
        "\n",
        "# Load model with mixed precision (float16)\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", torch_dtype=torch.float16)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = model.to(device)\n",
        "\n",
        "print_gpu_memory()\n",
        "\n",
        "# Optimizer with lower learning rate\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
        "\n",
        "# Initialize GradScaler for mixed precision\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Training loop with gradient accumulation and mixed precision\n",
        "epochs = 3\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Clear memory before each epoch\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        try:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision forward pass\n",
        "            with autocast():\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=input_ids,\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "\n",
        "            # Scale the loss for mixed precision\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Step the optimizer and scaler\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Print progress and clear memory periodically\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"CUDA out of memory error: {e}\")\n",
        "            # Clear memory and try to continue\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            continue\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {epoch_loss / len(data_loader)}\")\n",
        "\n",
        "# Save model\n",
        "model_save_path = \"/conversational_model\"\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"Model saved at {model_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "VzkhIajzxPBO",
        "outputId": "29c196ca-3e5e-4156-e5de-2ed84ab77644"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading tokenizer...\n",
            "Padding token set to: <|endoftext|>\n",
            "Loading dataset...\n",
            "Error: File not found at /root/.cache/kagglehub/datasets/grafstor/simple-dialogs-for-chatbot/versions/2/dialog.txt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/grafstor/simple-dialogs-for-chatbot/versions/2/dialog.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1532b1e9928f>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# Initialize dataset with smaller batch size and fewer samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# DataLoader with optimized batch size and workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1532b1e9928f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, tokenizer, max_length, max_samples)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mchat_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/grafstor/simple-dialogs-for-chatbot/versions/2/dialog.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "class ModelChatbot:\n",
        "    def __init__(self, model_path):\n",
        "        \"\"\"\n",
        "        Initialize the chatbot with a pre-trained model\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Path to the saved model directory\n",
        "        \"\"\"\n",
        "        # Determine device\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Load model and tokenizer\n",
        "        try:\n",
        "            self.model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "            # Ensure pad token is set\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            # Move model to device\n",
        "            self.model = self.model.to(self.device)\n",
        "            self.model.eval()  # Set to evaluation mode\n",
        "\n",
        "            print(\"Model loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def generate_response(self, prompt, max_length=100, temperature=0.7, top_k=50, top_p=0.95):\n",
        "        \"\"\"\n",
        "        Generate a response to the given prompt\n",
        "\n",
        "        Args:\n",
        "            prompt (str): Input text to generate response for\n",
        "            max_length (int): Maximum length of generated text\n",
        "            temperature (float): Controls randomness of predictions\n",
        "            top_k (int): Number of highest probability tokens to keep\n",
        "            top_p (float): Nucleus sampling probability threshold\n",
        "\n",
        "        Returns:\n",
        "            str: Generated response\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Tokenize input\n",
        "            input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "            # Generate response\n",
        "            with torch.no_grad():\n",
        "                output = self.model.generate(\n",
        "                    input_ids,\n",
        "                    max_length=max_length,\n",
        "                    num_return_sequences=1,\n",
        "                    no_repeat_ngram_size=2,\n",
        "                    temperature=temperature,\n",
        "                    top_k=top_k,\n",
        "                    top_p=top_p,\n",
        "                )\n",
        "\n",
        "            # Decode response\n",
        "            response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # Remove the original prompt from the response\n",
        "            response = response[len(prompt):].strip()\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating response: {e}\")\n",
        "            return \"I'm having trouble generating a response.\"\n",
        "\n",
        "    def interactive_chat(self):\n",
        "        \"\"\"\n",
        "        Start an interactive chat session with the model\n",
        "        \"\"\"\n",
        "        print(\"🤖 AI Chatbot Initialized! Type 'quit' to exit.\")\n",
        "        print(\"-------------------------------------------\")\n",
        "\n",
        "        # Chat loop\n",
        "        while True:\n",
        "            try:\n",
        "                # Get user input\n",
        "                user_input = input(\"You: \").strip()\n",
        "\n",
        "                # Check for exit command\n",
        "                if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "                    print(\"Goodbye! 👋\")\n",
        "                    break\n",
        "\n",
        "                # Generate and print response\n",
        "                if user_input:\n",
        "                    response = self.generate_response(user_input)\n",
        "                    print(\"AI: \" + response)\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\nChat interrupted. Type 'quit' to exit.\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "\n",
        "def main():\n",
        "    # Path to the saved model\n",
        "    model_path = \"/conversational_model\"  # Adjust this to your model's path\n",
        "\n",
        "    try:\n",
        "        # Create chatbot instance\n",
        "        chatbot = ModelChatbot(model_path)\n",
        "\n",
        "        # Start interactive chat\n",
        "        chatbot.interactive_chat()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to initialize chatbot: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o2Rd-Kfv1t8",
        "outputId": "065efd0e-5302-481d-e919-381760302611"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Model loaded successfully!\n",
            "🤖 AI Chatbot Initialized! Type 'quit' to exit.\n",
            "-------------------------------------------\n",
            "You: hii\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: i.\ti'm sorry for you.\n",
            "You: how are you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: u going to go?\ti'm going.\n",
            "You: dont go please\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: e.\ti'll go back upstairs anyway.\n",
            "You: what is upstairs?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: ?\ti'm having a party next saturday.\n",
            "You: am i invited?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI: ?\ti'm sorry for you.\n",
            "You: i love you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI: u to go to hell.\ti'll go back upstairs anyway.\n"
          ]
        }
      ]
    }
  ]
}